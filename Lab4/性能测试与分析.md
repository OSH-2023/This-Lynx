# 性能测试与分析

## 测试所用benchmark任务
在深度学习
我们选择的任务
```python
ray.init(dashboard_host="0.0.0.0")

size=[784,500,300,10]
nn=NN.remote(size)

# Launch four parallel square tasks.
futures = [nn.forward.remote(np.random.random(784)) for _ in range(10000)]

# Retrieve results.
print("Total number of results:",len(ray.get(futures)))
print("First 10 results:", ray.get(futures)[0:10])
```

（官网上有不少测试样例，同样可用于测试，有需要者可访问：https://docs.ray.io/en/latest/ray-air/benchmarks.html）


## 测试性能指标

- 资源使用率（如CPU、内存、GPU使用率）
- 吞吐量（单位时间处理的任务数）
- 单任务响应时间（每个任务从提交到完成的平均时间）
- 任务运行时调用的Actor数量（反映并行性）
- I/O与网络负载（I/O与网络的吞吐量大小）
- 任务执行成功率（没有特殊情况应该保证绝对地为100%）

我们将选择吞吐量和资源使用率作为性能测试指标。

## 性能测试结果与分析
### 单机部署


### 单机优化



### 分布式部署
我们以20000任务量，每组10个任务（函数参数规模过大时，会导致Ray报错，无法正常运行，故此处只能设为每组10个任务）
| 序号  | 用时/s | 吞吐量/任务数每秒 |
| :---: | :----: | :---------------: |
|   1   |  9.62  |                   |
|   2   | 11.06  |                   |
|   3   | 13.15  |                   |
|   4   |  8.68  |                   |
|   5   |  9.61  |                   |

为测得CPU使用率，我们应加大任务量至250000，以使Ray稳定运行一段时间（约2分钟），获得稳定的CPU使用率数据。
测得有效数据点如下：
<img src="src/" width="100%">
29.8
31.4
31.8
30.5
30.2
32.0


## 附：测试所用benchmark任务代码
```python
import ray
import numpy as np
import time

ray.init(dashboard_host="0.0.0.0")

def relu(x):
    if x>0:
        return x
    else:
        return 0.0

@ray.remote
class NN:#neural network
    def __init__(self,size):
        #搭建神经网络
        self.weight=[]#神经网络权重
        #self.sum_adjust_weight=[]#神经网络反向传播中对权重求导所得导数
        #self.layer=[]#神经网络输入层、各隐含层和输出层
        self.bias=[]#神经网络各层偏置
        #self.sum_adjust_bias=[]#神经网络反向传播中对偏置求导所得导数
        self.size=size#神经网络各层神经元数量

        self.total_loss=0.0#用来计算神经网络损失函数
        #self.total_correct=0
        for i in range(len(size)):
            #self.layer.append(np.zeros(size[i],dtype=np.float16))
            self.bias.append(np.zeros(size[i],dtype=np.float16))
        for i in range(len(size)-1):
            self.weight.append(np.random.normal(0,2.0/np.sqrt(size[i]+size[i+1]),size=(size[i],size[i+1])))#Xavier初始化
     
    
    def forward(self,layer_input):#由输入求神经网络输出结果
        x=np.array(layer_input)
        for i in range(1,len(size)):
            x=x@self.weight[i-1]
            for j in range(len(x)):
                x[j]=relu(x[j]+self.bias[i][j])
        
        return x
    
size=[784,500,300,10]
nn=NN.remote(size)

# Launch four parallel square tasks.
futures = [nn.forward.remote(np.random.random(784)) for _ in range(10000)]

# Retrieve results.
print("Total number of results:",len(ray.get(futures)))
print("First 10 results:", ray.get(futures)[0:10])
```





