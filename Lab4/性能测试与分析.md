# 性能测试与分析



## 测试所用benchmark任务
```python
ray.init(dashboard_host="0.0.0.0")

size=[784,500,300,10]
nn=NN.remote(size)

# Launch four parallel square tasks.
futures = [nn.forward.remote(np.random.random(784)) for _ in range(10000)]

# Retrieve results.
print("Total number of results:",len(ray.get(futures)))
print("First 10 results:", ray.get(futures)[0:10])
```




（官网上有不少测试样例，同样可用于测试，有需要者可访问：https://docs.ray.io/en/latest/ray-air/benchmarks.html）


## 性能指标





## 性能测试结果与分析




## 附：测试所用benchmark任务代码
```python
import ray
import numpy as np
import time

ray.init(dashboard_host="0.0.0.0")

def relu(x):
    if x>0:
        return x
    else:
        return 0.0

@ray.remote
class NN:#neural network
    def __init__(self,size):
        #搭建神经网络
        self.weight=[]#神经网络权重
        #self.sum_adjust_weight=[]#神经网络反向传播中对权重求导所得导数
        #self.layer=[]#神经网络输入层、各隐含层和输出层
        self.bias=[]#神经网络各层偏置
        #self.sum_adjust_bias=[]#神经网络反向传播中对偏置求导所得导数
        self.size=size#神经网络各层神经元数量

        self.total_loss=0.0#用来计算神经网络损失函数
        #self.total_correct=0
        for i in range(len(size)):
            #self.layer.append(np.zeros(size[i],dtype=np.float16))
            self.bias.append(np.zeros(size[i],dtype=np.float16))
        for i in range(len(size)-1):
            self.weight.append(np.random.normal(0,2.0/np.sqrt(size[i]+size[i+1]),size=(size[i],size[i+1])))#Xavier初始化
     
    
    def forward(self,layer_input):#由输入求神经网络输出结果
        x=np.array(layer_input)
        for i in range(1,len(size)):
            x=x@self.weight[i-1]
            for j in range(len(x)):
                x[j]=relu(x[j]+self.bias[i][j])
        
        return x
    
size=[784,500,300,10]
nn=NN.remote(size)

# Launch four parallel square tasks.
futures = [nn.forward.remote(np.random.random(784)) for _ in range(10000)]

# Retrieve results.
print("Total number of results:",len(ray.get(futures)))
print("First 10 results:", ray.get(futures)[0:10])
```





